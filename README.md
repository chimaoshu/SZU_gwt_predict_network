# 基于深度学习预测深大公文的受欢迎程度

## 使用过程

#### 使用方法：
1、打开MATLAB

2、运行"Train .m"进行训练，或者直接打开读取训练500次后的数据 "训练后存储的数据.mat"

3、运行"Test.m"进行测试，输入公文标题，即可得出预测结果



#### 注意事项：
1、您可以在"Train.m"代码中自定义训练次数

2、在进行训练时，如果您的电脑里面有NVIDIA的**CUDA**和MATLAB的**parallel computing toolbox**，则可以在"Train.m"大越60行处，取消一行代码的注释，在"ReNewWeights.m"大约10行处，取消四行代码的注释。这样就可以使用GPU进行并行计算，加快训练速度。

3、运行"Test.m"时，MATLAB会调用Python函数，获取输入数据。如果您的MATLAB版本较低，不支持在MATLAB中直接调用Python，那么请运行”GenerateInputVector.py“，然后按照指引把输出结果复制后，运行"Test2.m"并粘贴，最后能和直接运行"Test.m"得到相同的结果。

4、关于”GenerateInputVector.py“中需要用到的Python库：
hashlib、time、json、random、urllib 都是官方标准库
**唯一需要额外安装的是requests：** `pip install requests`

5、由于MATLAB针对Python的读取机制，如果您想要修改Python脚本的内容并让他生效，需要运行一次"**ReloadPythonModule.m**"。（我被这个坑了好久）



## 开发与设计

#### 关于数据的处理

​		如果您想要训练自己的数据，请注意，训练数据存储在**"数据处理/处理到最后投入使用的数据/训练数据.json"**。这个json由很多个list构成，每个list就是一组训练数据，其中每个list的第一个数代表某则公文的预期点击量，后面的所有数字代表这个公文标题含有的关键词。

​		至于数字和关键词如何对应起来，请查看**"数据处理/处理到最后投入使用的数据/输入神经元与序号.json"**，其中记录了每一个关键词和对应的数据。

​		一共有1534的关键词，他们是我统计出的，从2013年到2020年7月中，出现频率排名前1534的关键词（正好是7年内出现次数大于20的关键词）。关于这些数据如何得到，可以查看**“数据处理/输入神经元的数据处理”**中的处理脚本。至于最原始数据的获得，与我的另一个开源小项目有关：[https://github.com/chimaoshu/SZU_gwt_trend_analysis](https://github.com/chimaoshu/SZU_gwt_trend_analysis) 。这1534个关键词，正好对应神经网络的**1534个输入神经元**，输入时用0和1表示这些关键词的出现与否。

​		训练用的数据原本是爬取了从2013年到2020年7月的所有含有正常点击量的公文（学校2017和2018年的公文点击量出现大量异常），后来发现一件事：由于前面13、14、15年网络不如现在发达，因此当时公文的数量普遍偏低，导致训练出来的结果非常不如意。后来决定只爬2019至2020年7月的公文，并且剔除掉了与平均值相差较大的数据，本来**两万多组**训练数据精选到最后只剩下**2160组**。

#### 关于深度神经网络的设计

​		这是我第一次从零开始设计深度神经网络，这个简单的全连接神经网络花了我将近两天的时间进行调参，才有了较好的结果。

这是一个三层的深度神经网络，一开始我使用的是四层的神经网络，但是那样子的误差总是不能很有效地反馈给第一层（和训练数据的数量小有关），经过很多次试验，效果最好的就是一个三层的神经网络。

第一层有





