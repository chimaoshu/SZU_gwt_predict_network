# 基于深度学习预测深大公文的受欢迎程度

## 效果

输入一则公文的标题，神经网络会给出预计的公文点击数。

## 使用过程

#### 使用方法：
1、打开MATLAB

2、运行"Train .m"进行训练，或者直接读取 "训练后存储的数据.mat"

3、运行"Test.m"进行测试，输入公文标题，即可得出预测结果



#### 注意事项：
1、您可以在"Train.m"代码中自定义训练次数

2、在进行训练时，如果您的电脑里面有NVIDIA的**CUDA**和MATLAB的**parallel computing toolbox**，则可以在"Train.m"大越60行处，取消一行代码的注释，在"ReNewWeights.m"大约10行处，取消四行代码的注释。这样就可以使用GPU进行并行计算，加快训练速度。

3、运行"Test.m"时，MATLAB会调用Python函数，获取输入数据。如果您的MATLAB版本较低，不支持在MATLAB中直接调用Python，那么请运行”GenerateInputVector.py“，然后按照指引把输出结果复制后，运行"Test2.m"并粘贴，最后能和直接运行"Test.m"得到相同的结果。

4、关于”GenerateInputVector.py“中需要用到的Python库：
hashlib、time、json、random、urllib 都是官方标准库
**唯一需要额外安装的是requests：** `pip install requests`

5、由于MATLAB针对Python的读取机制，如果您想要修改Python脚本的内容并让他生效，需要运行一次"**ReloadPythonModule.m**"。（我被这个坑了好久）



## 开发与设计

#### 关于数据的处理

如果您想要训练自己的数据，请注意，训练数据存储在"**数据处理/处理到最后投入使用的数据/训练数据.json**"。这个json由很多个list构成，每个list就是一组训练数据，其中每个list的第一个数代表某则公文的预期点击量，后面的所有数字代表这个公文标题含有的关键词。

至于数字和关键词如何对应起来，请查看"**数据处理/处理到最后投入使用的数据/输入神经元与序号.json**"，其中记录了每一个关键词和对应的数据。

一共有1534的关键词，他们是我统计出的，从2013年到2020年7月中，出现频率排名前1534的关键词（正好是7年内出现次数大于20的关键词）。关于这些数据如何得到，可以查看“**数据处理/输入神经元的数据处理**”中的处理脚本。至于最原始数据的获得，与我的另一个小项目有关：[https://github.com/chimaoshu/SZU_gwt_trend_analysis](https://github.com/chimaoshu/SZU_gwt_trend_analysis) 。这1534个关键词，正好对应神经网络的**1534个输入神经元**，输入时用0和1表示这些关键词的出现与否。

训练用的数据原本是爬取了从2013年到2020年7月的所有含有正常点击量的公文（学校2017和2018年的公文点击量出现大量异常），后来发现一件事：可能是因为13、14、15年网络不如现在发达，当时公文的点击量普遍偏低，导致训练出来的结果非常不如意。后来决定只爬2019至2020年7月的公文，并且剔除掉了与平均值相差较大的数据，本来**两万多组**训练数据精选到最后只剩下**2160组**。

#### 关于深度神经网络的设计

这是我第一次从零开始设计深度神经网络，这个简单的全连接神经网络花了我几天的时间进行调参，才有了较好的结果。

##### 网络结构

这是一个三层的深度神经网络，一开始我使用的是四层的神经网络，但试了各种参数，误差总是不能很有效地反馈给第一层，可能和训练数据的数量小有关。经过很多次试验，效果最好的还是一个三层的神经网络。

**第一层**有1534个输入神经元，对应的是经过统计后，近5年来出现在公文标题中次数大于20次的1534个关键词。

**第二层**有300个神经元。

而**第三层**，即输出层，只有1个神经元，其含义是神经网络对于公文点击量给出的预测结果。

##### 激活函数

我尝试了各种各样的激活函数组合，最终效果最好的，是两层激活函数均使用LeakyReLU。
使用Sigmoid等激活函数容易出现梯度消失的问题，由于预测的结果是公文的点击数，所以loss一般都是几十甚至几百，Sigmoid等函数不能有效保留原来的梯度。

而使用ReLU函数则会因为初始变化较大而导致大量神经元死亡，调低学习率也拯救不了，也放弃了ReLU。

最终的选择是使用Leaky ReLU，既能保留原有梯度，又能有效防止神经元死亡的问题。最终函数的第一层使用Leaky ReLU做激活函数，而第二层输出不使用激活函数。

##### 学习率

刚开始一直使用每训练一次学习率减半的做法，后来才知道这样做是很蠢的，学习率偏大或者偏小都会导致loss变大。尝试了很多种方法，最终决定是学习率固定在 alpha = 0.0001 效果比较好。

由于学习率不随着训练程度进行变化，所以训练到一定程序有可能出现误差不降反增的情况，于是设定了一个预期误差，达到条件就停止学习。

##### 权重的初始化
一开始完全不懂神经网络，就想拿每个关键词在这么多年来的点击数平均值来给矩阵做初始化，后来尝试后就知道这个想法是真的离谱。查看了网上各种各样的初始化方法，都感觉有些麻烦，最终尝试使用rand和zeros两种简单的初始化方法，发现rand的效果更好。
于是就采用了rand对权值进行初始化。

##### 优化算法

采用随机梯度下降，进行反向传播。没有使用过其他的优化算法，因为我唯一会用的优化算法只有随机梯度下降。。。


##### 2020-7-3