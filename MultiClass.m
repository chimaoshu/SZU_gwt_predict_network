function [W1,W2] = MultiClass(W1,W2,X,D)
    
    alpha = 0.9;
    
    N = 5;
    for k=1:N
        
        % 5x5 matrix to 25x1 matrix
        x = reshape(X(:,:,k),25,1);
        
        % get vector and transform，d是正确值
        %D(k,:)的意思是让矩阵第k行的元素等于向量d的元素
        % 然后转置为一列向量
        d = D(k,:)';
        
        % 第一层线性组合
        % W1是一个50行25列的矩阵
        % x是25维的列向量
        % v1是50维的列向量
        % 这里相乘是矩阵的外积，结果仍为矩阵
        % W1的第一行（一个25维的行向量）乘以输入的X（25维列向量）
        % 得到第二个网络层的第一个神经元
        % W1的第一行就是输出的25个神经元对第二层的第一个神经元的25个权重
        v1 = W1 * x;
        % 第一层输出v1（一整个列向量）
        y1 = Sigmoid(v1);
        
        % 隐藏层
        v2 = W2 * y1;
        y2 = Sigmoid(v2);
        
        % x是输入，y1是中间隐藏层，y2是最终的输出
        % 前向传递end
        % 接下来是后向传递
        
        %计算loss，这里采用直接相减
        % 这里e就是目标函数（误差）
        % 后面要做的事就是让e下降
        e = d-y2;
        delta1 = e;
        
        % e1等于误差乘以W2的转置
        % （前面正向传播是乘以W2，这里反向传播就是乘以W2的转置变回去）
        %  这个是隐藏层的目标函数，不像前面一样直接相减
        e1 = W2'*delta1;
        % 这里是固定公式，求出隐藏层的误差delta2
        delta2 = y1.*(1-y1).*e1;
        
        % 输出对于W2的反向传播与更新
        dW2 = alpha * delta1 * y1';
        W2 = W2 + dW2;
        
        % 更新权重
        dW1 = alpha * delta2 * x';
        W1 = W1 + dW1;

    end
end

